{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, areas marked FFF should be completed by you.\n",
    "\n",
    "\n",
    "Frozen lake is an OpenAI Gym environment wherein the agent must navigate across a grid, representing a frozen lake. However, several of the squares on this lake are not sufficiently frozen, and stepping on them will send the agent to its death, incurring a negative reward.\n",
    "\n",
    "The agent must be trained to navigate this frozen lake, avoiding the squares where falling will send it to death, while learning to reach the goal state at the end.\n",
    "\n",
    "A visualization of the environment is here:\n",
    "https://twice22.github.io/images/rl_series/frozenlake.png\n",
    "\n",
    "The actions in the environment will move the agent up, down, left, or right on the grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install required dependencies\n",
    "\n",
    "pip install gym\n",
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, areas marked FFF should be completed by you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Discrete(4)\n",
      "Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "\n",
    "np.random.seed(42)\n",
    "env.action_space.seed(42)\n",
    "env.observation_space.seed(42)\n",
    "\n",
    "obs, info = env.reset()\n",
    "print(obs)\n",
    "\n",
    "# This shows the action space is 4 dimensional and discrete.\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "\n",
    "NStates = env.observation_space.n\n",
    "NActions = env.action_space.n\n",
    "\n",
    "# To actually move the env forward, we must tell it what action should be taken.\n",
    "action = 1\n",
    "next_obs, reward, terminated, truncated, info = env.step(action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the Q-table, the main object of Q-learning.\n",
    "This table has rows given by the states and columns given by actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((NStates, NActions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the table, we can instantly look up the Q value of being\n",
    "in state s and taking action a by calling\n",
    "\n",
    "q_table[state, action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Brody1/anaconda3/lib/python3.9/site-packages/gym/envs/toy_text/frozen_lake.py:271: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"FrozenLake-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample() # sample a random action\n",
    "next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(env.render()) # print your current position in the maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some common hyper-params\n",
    "\n",
    "# total training episodes\n",
    "epochs = 1000\n",
    "# learning rate\n",
    "alpha = 0.5\n",
    "# discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Threshold for taking random actions.\n",
    "eps = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reached_goal = []\n",
    "running_success_rates = []\n",
    "x_range = []\n",
    "convergence_rate = None\n",
    "epsilon_greedy_threshold = 0.5\n",
    "greedy = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the actual training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(epochs):\n",
    "    obs, info = env.reset()\n",
    "    terminated = False\n",
    "    \n",
    "    reached_goal.append(0)\n",
    "\n",
    "    while not terminated:\n",
    "        # keep going until you fall into the ice or reach the goal\n",
    "\n",
    "        # these are the values of every possible action at the given observation.\n",
    "        action_values = q_table[obs]\n",
    "\n",
    "        # if all action values are 0, the q-table will always take the same actions.\n",
    "        # this is just making sure we take random actions at the start of training.\n",
    "        if np.max(action_values) < eps:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        else:\n",
    "            if greedy is False:\n",
    "                action = np.argmax(action_values)\n",
    "            else:\n",
    "                if np.random.rand() < epsilon_greedy_threshold:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(action_values)\n",
    "                \n",
    "\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        q_table[obs, action] = q_table[obs, action] \\\n",
    "                               + alpha * (reward + gamma * np.max(q_table[next_obs]) - q_table[obs, action])\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        # we reached the goal\n",
    "        if reward:\n",
    "            # log the success\n",
    "            reached_goal[-1] = 1\n",
    "\n",
    "        #\n",
    "    if ep % 10 == 0 and ep > 0:\n",
    "        # Compute running success rate.\n",
    "        # the average success over the last\n",
    "        # 10 epochs.\n",
    "        # for example, if you succeeded once,\n",
    "        # this would be given by 1/10\n",
    "        average_success_rate_last_10 = np.mean(reached_goal[-10:-1])\n",
    "        if average_success_rate_last_10 > .95 and convergence_rate is None:\n",
    "            convergence_rate = ep\n",
    "        running_success_rates.append(average_success_rate_last_10)\n",
    "        x_range.append(ep)\n",
    "        if epsilon_greedy_threshold > 0:\n",
    "            epsilon_greedy_threshold -= .05\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3051837  0.         0.59049    0.        ]\n",
      " [0.22473688 0.         0.6561     0.29521599]\n",
      " [0.14358595 0.729      0.         0.33623048]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.81       0.         0.48634569]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.9        0.         0.36449468]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.68396896]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please answer the following questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #1 Fill in the states and actions.\n",
    "\n",
    "How many states are there? How many actions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 states, 4 actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #2 Complete the Q-learning update, which has been only partially filled in on line 22 and 23 above.\n",
    "\n",
    "After training, please print the final Q-table at convergence. Does this Q-table make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does make sense. Essentially, the algorithm learned a successful path and then never strayed from that path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #3 Plot the learning curve\n",
    "\n",
    "On the x-axis, plot the total number of training episodes. On the y-axis, plot the average success rate over the last 10 episodes. An episode is a success if the goal is reached and the agent gets a reward of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_range, running_success_rates, label='Learning curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #4 Play around with alpha and gamma.\n",
    "\n",
    "Suppose we define the rate of convergence to be the number of episodes it takes for 95% of the episodes to get a reward.\n",
    "\n",
    "Change the learning rate alpha and the discount factor gamma. Try values around 0.7 for alpha and 0.2 for gamma. What happens to the rate of convergence? What does the learning curve look like? What are good values for alpha and gamma?\n",
    "\n",
    "You need not be rigorous here. Just a brief discussion on a few different values you tried will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convergence_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha = 0.5, gamma = 0.9, convergence rate = 90\n",
    "alpha = 0.7, gamma = 0.2, convergence rate = 150\n",
    "alpha = 0.2, gamma = 0.7, convergence rate = 90\n",
    "alpha = 0.9, gamma = 0.1, convergence rate = 510\n",
    "alpha = 0.9, gamma = 0.9, convergence rate = 90\n",
    "alpha = 0.1, gamma = 0.1, convergence rate > 1000\n",
    "alpha = 0.1, gamma = 0.9, convergence rate = 130"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #5 Add epsilon greedy exploration\n",
    "\n",
    "Rather than always taking the action with the max q-value on line 17 above, implement epsilon greedy exploration, which we discussed in class.\n",
    "\n",
    "Crucially, you should anneal your epsilon greedy threshold throughout training. It should start around 0.5, and then decrease by around 0.05 every 10 episodes. This schedule need not be exact.\n",
    "\n",
    "When I say it should start around 0.5, I mean that you should take a random action half the time. Decreasing by 0.05 means that after 10 episodes, you should only take a random action 45% of the time, then after 10 more episodes 40% of the time, and so on.\n",
    "\n",
    "After implementing epsilon greedy exploration, what happens to the Q-table? Compare to the Q-table you got in part 3, when you did not use epsilon greedy exploration. They should be different. Can you interpret the difference? What does it mean, in terms of paths the agent might take?\n",
    "\n",
    "Compared with no epsilon greedy exploration, how fast is the rate of convergence? Show the learning curve for the epsilon greedy case, similar to the one from 3.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c2fc23ecdc1c1fa286c1002cc3fcf9afea5e395bec68affde10a5bf206900e71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
