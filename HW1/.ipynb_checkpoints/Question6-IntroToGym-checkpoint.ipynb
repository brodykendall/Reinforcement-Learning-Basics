{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to GYM and Cartpole\n",
    "\n",
    "This question serves as an introduction to the OpenAI Gym API.\n",
    "\n",
    "To begin with, please install gym with the command: pip install gym\n",
    "\n",
    "Gym is a set of classical RL environments and problems that were once used for RL reserach.\n",
    "\n",
    "Today, these environemnts are mostly unused in research. However, the general API established in the library continues to live on.\n",
    "\n",
    "In the next several problems, we will use Gym on Cartpole, a classical RL environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00706754 -0.03659857 -0.02178115 -0.04450863]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# To make output consistent\n",
    "np.random.seed(42)\n",
    "\n",
    "# This makes the cartpole env\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "env.action_space.seed(42)\n",
    "env.observation_space.seed(42)\n",
    "\n",
    "obs, info = env.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# This shows the action space is 2 dimensional and discrete.\n",
    "print(env.action_space)\n",
    "\n",
    "# For discrete action spaces, the action is given as an integer in the range [0, action-range - 1]\n",
    "# For the case of cartpole, this means the action is an integer in the range [0, 1]\n",
    "# 0 pushes the cart to the left and 1 pushes the cart to the right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To actually move the env forward, we must tell it what action should be taken.\n",
    "action = 0\n",
    "next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# We see that stepping forward with env.step() returns the next observation, i.e. the result\n",
    "# of taking the given action in the environment. It also returns the reward in the environment,\n",
    "# and the terminated flag which tells us if the episode has ended.\n",
    "# Common reasons for the episode to end are succeeding or failing a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin_policy(obs):\n",
    "    # Generally a policy will take some function of the observation and return an action.\n",
    "    # For example:\n",
    "    sin_obs = np.sin(obs[1])\n",
    "    policy_action = None\n",
    "    if sin_obs > 0:\n",
    "        policy_action = 0\n",
    "    else:\n",
    "        policy_action = 1\n",
    "    return policy_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0318456   0.0431256  -0.03319371  0.04301013]\n",
      "[-0.03098308 -0.15150502 -0.0323335   0.3250381 ]\n",
      "Made it to step: 0\n",
      "[-0.03401319  0.04406203 -0.02583274  0.02233646]\n",
      "Made it to step: 1\n",
      "[-0.03313195 -0.15068012 -0.02538601  0.30675822]\n",
      "Made it to step: 2\n",
      "[-0.03614555  0.0447942  -0.01925085  0.00617853]\n",
      "Made it to step: 3\n",
      "[-0.03524967 -0.15004645 -0.01912728  0.29272595]\n",
      "Made it to step: 4\n",
      "[-0.03825059  0.04534292 -0.01327276 -0.00592756]\n",
      "Made it to step: 5\n",
      "[-0.03734373 -0.14958619 -0.01339131  0.28253826]\n",
      "Made it to step: 6\n",
      "[-0.04033546  0.04572418 -0.00774055 -0.01433795]\n",
      "Made it to step: 7\n",
      "[-0.03942097 -0.14928591 -0.0080273   0.27589273]\n",
      "Made it to step: 8\n",
      "[-0.04240669  0.04594965 -0.00250945 -0.01931117]\n",
      "Made it to step: 9\n",
      "[-0.0414877  -0.14913623 -0.00289567  0.27257895]\n",
      "Made it to step: 10\n",
      "[-0.04447043  0.04602692  0.0025559  -0.02101588]\n",
      "Made it to step: 11\n",
      "[-0.04354988 -0.1491316   0.00213559  0.27247235]\n",
      "Made it to step: 12\n",
      "[-0.04653252  0.04595982  0.00758503 -0.01953623]\n",
      "Made it to step: 13\n",
      "[-0.04561332 -0.14927009  0.00719431  0.2755302 ]\n",
      "Made it to step: 14\n",
      "[-0.04859872  0.04574849  0.01270491 -0.014875  ]\n",
      "Made it to step: 15\n",
      "[-0.04768375 -0.14955334  0.01240741  0.28178924]\n",
      "Made it to step: 16\n",
      "[-0.05067482  0.04538945  0.0180432  -0.00695475]\n",
      "Made it to step: 17\n",
      "[-0.04976703 -0.14998655  0.0179041   0.29136598]\n",
      "Made it to step: 18\n",
      "[-0.05276676  0.0448756   0.02373142  0.00438312]\n",
      "Made it to step: 19\n",
      "[-0.05186925 -0.15057851  0.02381909  0.3044581 ]\n",
      "Made it to step: 20\n",
      "[-0.05488082  0.04419603  0.02990825  0.01938132]\n",
      "Made it to step: 21\n",
      "[-0.0539969  -0.15134178  0.03029587  0.32134858]\n",
      "Made it to step: 22\n",
      "[-0.05702373  0.04333593  0.03672285  0.03837165]\n",
      "Made it to step: 23\n",
      "[-0.05615702 -0.15229286  0.03749028  0.34241107]\n",
      "Made it to step: 24\n",
      "[-0.05920287  0.04227621  0.0443385   0.06178211]\n",
      "Made it to step: 25\n",
      "[-0.05835735 -0.15345249  0.04557414  0.36811787]\n",
      "Made it to step: 26\n",
      "[-0.0614264   0.04099327  0.0529365   0.09014624]\n",
      "Made it to step: 27\n",
      "[-0.06060654 -0.15484592  0.05473942  0.39904958]\n",
      "Made it to step: 28\n",
      "[-0.06370346  0.0394585   0.06272042  0.12411486]\n",
      "Made it to step: 29\n",
      "[-0.06291428 -0.15650332  0.06520271  0.43590733]\n",
      "Made it to step: 30\n",
      "[-0.06604435  0.03763795  0.07392086  0.16447026]\n",
      "Made it to step: 31\n",
      "[-0.06529159 -0.15846007  0.07721026  0.47952694]\n",
      "Made it to step: 32\n",
      "[-0.06846079  0.03549184  0.08680081  0.2121431 ]\n",
      "Made it to step: 33\n",
      "[-0.06775095 -0.16075699  0.09104367  0.5308955 ]\n",
      "Made it to step: 34\n",
      "[-0.07096609  0.03297429  0.10166158  0.268232  ]\n",
      "Made it to step: 35\n",
      "[-0.07030661 -0.16344047  0.10702621  0.59117   ]\n",
      "Made it to step: 36\n",
      "[-0.07357541  0.03003315  0.11884961  0.33402577]\n",
      "Made it to step: 37\n",
      "[-0.07297476 -0.16656214  0.12553012  0.66169804]\n",
      "Made it to step: 38\n",
      "[-0.076306    0.0266103   0.1387641   0.41102836]\n",
      "Made it to step: 39\n",
      "[-0.07577379 -0.170178    0.14698465  0.74403924]\n",
      "Made it to step: 40\n",
      "[-0.07917735  0.02264269  0.16186544  0.500985  ]\n",
      "Made it to step: 41\n",
      "[-0.0787245  -0.17434649  0.17188515  0.8399872 ]\n",
      "Made it to step: 42\n",
      "[-0.08221143  0.01806451  0.1886849   0.6059095 ]\n",
      "Made it to step: 43\n",
      "[-0.08185013 -0.17912489  0.20080309  0.95158875]\n",
      "Made it to step: 44\n",
      "[-0.08543263  0.01281153  0.21983485  0.72810924]\n",
      "Made it to step: 45\n"
     ]
    }
   ],
   "source": [
    "# Usually, we use the policy to write a for loop over the environment, moving it forward according to the policy.\n",
    "obs, info = env.reset()\n",
    "print(obs)\n",
    "horizon = 500\n",
    "terminated = False\n",
    "for i in range(horizon):\n",
    "    if terminated is False:\n",
    "        action = sin_policy(obs)\n",
    "        # action = random_policy(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        print(next_obs)\n",
    "        print(F\"Made it to step: {i}\")\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The full code for cartpole is here:\n",
    "# https://github.com/openai/gym/blob/master/gym/envs/classic_contro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please answer the following questions.\n",
    "\n",
    "\n",
    "# 1. Implement the function random_policy\n",
    "\n",
    "This policy takes the observation, ignores it, and returns a random action. It should sample the actions uniformly, with equal probability of any action.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(obs):\n",
    "    # Takes an observation and returns a random action.\n",
    "    policy_action = randint(0,1)\n",
    "    return policy_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Compare the performance of sin_policy and random policy.\n",
    "\n",
    "Do this by running both policies 100 times until termination. Then report the mean and variance of the number of steps achieved by each policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41.43, 284.12636363636364]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sin_policy:\n",
    "\n",
    "num_steps = []\n",
    "for j in range(100):\n",
    "    obs, info = env.reset()\n",
    "    horizon = 500\n",
    "    terminated = False\n",
    "    for i in range(horizon):\n",
    "        if terminated is False:\n",
    "            action = sin_policy(obs)\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            obs = next_obs\n",
    "        else:\n",
    "            num_steps.append(i)\n",
    "            break\n",
    "            \n",
    "[stat.mean(num_steps), stat.variance(num_steps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24.71, 196.57161616161616]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random policy:\n",
    "\n",
    "num_steps = []\n",
    "for j in range(100):\n",
    "    obs, info = env.reset()\n",
    "    horizon = 500\n",
    "    terminated = False\n",
    "    for i in range(horizon):\n",
    "        if terminated is False:\n",
    "            action = random_policy(obs)\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            obs = next_obs\n",
    "        else:\n",
    "            num_steps.append(i+1)\n",
    "            break\n",
    "            \n",
    "            \n",
    "[stat.mean(num_steps), stat.variance(num_steps)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Try to devise a hand-coded policy that beats sin_policy.\n",
    "\n",
    "This policy should NOT use any machine learning. It should be a determinstic function of the input. Report the mean and variance of your policy across 100 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_policy(obs):\n",
    "    x, v, theta, v_theta = obs\n",
    "    sin_obs = np.sin(theta + .1*v_theta)\n",
    "    policy_action = None\n",
    "    if sin_obs > 0:\n",
    "        policy_action = 1\n",
    "    else:\n",
    "        policy_action = 0\n",
    "    return policy_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[500, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_steps = []\n",
    "horizon = 500\n",
    "for j in range(100):\n",
    "    obs, info = env.reset()\n",
    "    terminated = False\n",
    "    for i in range(horizon):\n",
    "        if terminated is False:\n",
    "            action = my_policy(obs)\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            obs = next_obs\n",
    "        else:\n",
    "            num_steps.append(i+1)\n",
    "            break\n",
    "        if i+1 == horizon:\n",
    "            num_steps.append(i+1)\n",
    "            \n",
    "            \n",
    "[stat.mean(num_steps), stat.variance(num_steps)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
